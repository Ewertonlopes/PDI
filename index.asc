:source-highlighter: pygments
:toc: left
:stem:

= Atividades Práticas Processamento Digital de Imagens =
Ewerton Vasoncelos Lopes <ewerton.lopes.140@ufrn.edu.br>

O relatório a seguir diz respeito a todas atividades referentes às práticas desenvolvidas para a disciplina de Processamento Digital de Imagens - DCA0445 durante o período de 2023.2 na Universidade Federal do Rio Grande do Norte.

== ------------ Unidade 1 ------------ 

Abaixo temos as atividades desenvolvidas para a unidade 1 da disciplina de Processamento Digital de Imagens.

== Atividade 1: Manipulando Pixels

As primeiras atividades possuem um caráter introdutório para trabalhar o desenvolvimento da manipulação dos pixels de uma dada imagem qualquer. Para desenvolver essa competência se utilizou do desenvolvimento de um negativo de uma área de uma dada imagem de entrada e a reordenação de uma imagem em seus quadrantes.

=== Criando Negativo

O desenvolvimento de um negativo de uma área é feito de forma simples com uma entrada de uma imagem qualquer pelo argumento de linha de comando e uma entrada cin posterior de dois pontos.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

int main(int argc, char** argv)
{
    cv::Mat image;
    cv::Vec3b val;
    
    cv::Point p1,p2;

    image = cv::imread(argv[1],cv::IMREAD_GRAYSCALE);
    
    do
    {
        std::cout<<"Ponto Inicial (x y):"; 
        std::cin>>p1.x>>p1.y;
    }while(p1.x > image.rows || p1.y > image.cols);

    do
    {
        std::cout<<"Ponto Final (x y):";
        std::cin>>p2.x>>p2.y;
    }while(p2.x > image.rows || p2.y > image.cols);

    int xin, xout, yin, yout;
    if(p1.x > p2.x){xin = p2.x; xout = p1.x;}
    else{xin = p1.x; xout = p2.x;} 

    if(p1.y > p2.y){yin = p2.y; yout = p1.y;}
    else {yin = p1.y; yout = p2.y;}

    for(int i = xin; i<=xout; i++)
    {
        for(int j = yin; j<yout; j++)
        {
            image.at<uchar>(i,j) = 255 - image.at<uchar>(i,j);
        }
    }

    cv::imshow("Negative", image);
    cv::waitKey();

    return 0;
}
----

A geração do negativo pode ser evidenciado pela parte do código logo abaixo onde se retira o valor da posição de cada pixel do valor de 255 no caso de usarmos um unsigned char para a geração da imagem.

[source,cpp]
----
for(int i = xin; i<=xout; i++)
{
    for(int j = yin; j<yout; j++)
    {
        image.at<uchar>(i,j) = 255 - image.at<uchar>(i,j);
    }
}
----

O resultado obtido pode ser visto na figura <<fig_neg>>.

[[fig_neg, Negativo]]
.Saída do programa negativo
image::images/negative.png[title="Execução do Programa em Negativo"]


=== Invertendo Quadrantes

O processo de inverter quadrantes segue uma lógica muito parecida com a que foi aplicada na atividade anterior. Apenas dessa vez devemos fazer isso de forma fixa a depender das dimensões da imagem. Porém, como forma de aprofundar os conhecimentos da biblioteca do openCV, se resolveu utilizar outros métodos internos a api sem o uso de loops externos e acessos aos pixels individuais. Como podemos ver no código logo abaixo a imagem foi repartida em quatro pedaços menores que a posteriori foram unidos nas posições corretas.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

int main(int argc, char** argv)
{
    cv::Mat image;
    cv::Vec3b val;
    
    image = cv::imread(argv[1],cv::IMREAD_GRAYSCALE);
   
    cv::Mat sub11out = image(cv::Range(0,(image.rows/2) - 1), cv::Range(0,(image.cols/2) - 1));
  cv::Mat sub12out = image(cv::Range(0,(image.rows/2) - 1), cv::Range(image.cols/2,image.cols));
    cv::Mat sub21out = image(cv::Range(image.rows/2,image.rows), cv::Range(0,(image.cols/2)));
  cv::Mat sub22out = image(cv::Range(image.rows/2,image.rows), cv::Range(image.cols/2,image.cols));

    cv::Mat aux = image.clone();

    sub22out.copyTo(aux(cv::Rect(0,0,sub22out.cols, sub22out.rows)));
    sub12out.copyTo(aux(cv::Rect(0,image.cols/2,sub12out.cols, sub12out.rows)));
    sub21out.copyTo(aux(cv::Rect(image.rows/2,0,sub21out.cols, sub21out.rows)));
    sub11out.copyTo(aux(cv::Rect(image.rows/2,image.cols/2,sub11out.cols, sub11out.rows)));
    
    cv::imshow("Inverse", aux);
    cv::waitKey();

    return 0;
}
----

O resultado obtido pode ser visto na figura <<fig_quad>>.

[[fig_quad, Quadrants]]
.Saída do programa Quadrantes
image::images/quadrants.png[title="Saída do Programa Quadrantes."]


== Atividade 2: Salvando Imagens

A atividade seguinte foi desenvolvida para melhorar a compreensão do estudante quanto às formas de se guardar as imagens em um computador. Abaixo temos o código desenvolvido nesta atividade.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>
#include <sstream>
#include <string>

int SIDE = 256;
int PERIODOS = 4;

int main(int argc, char** argv) 
{
  std::stringstream ss_img, ss_yml;
  cv::Mat image;

  ss_yml << "senoide-" << SIDE << ".yml";
  image = cv::Mat::zeros(SIDE, SIDE, CV_32FC1);

  cv::FileStorage fs(ss_yml.str(), cv::FileStorage::WRITE);

  for (int i = 0; i < SIDE; i++) {
    for (int j = 0; j < SIDE; j++) {
      image.at<float>(i, j) = 127 * sin(2 * M_PI * PERIODOS * j / SIDE) + 128;
    }
  }

  fs << "mat" << image;
  fs.release();

  cv::normalize(image, image, 0, 255, cv::NORM_MINMAX);
  image.convertTo(image, CV_8U);
  ss_img << "senoide-" << SIDE << ".png";
  cv::imwrite(ss_img.str(), image);

  fs.open(ss_yml.str(), cv::FileStorage::READ);
  fs["mat"] >> image;

  cv::normalize(image, image, 0, 255, cv::NORM_MINMAX);
  image.convertTo(image, CV_8U);

  cv::imshow("image", image);
  cv::waitKey();
}
----

Ao abrirmos os arquivos resultantes temos um arquivo que só pode ser compreendido por um visualizador de imagens gerado pelo png, como demonstra a figura <<fig_png>>, e um arquivo que pode ser facilmente compreendido por seres humanos no arquivo yml, sem contar as incontáveis possibilidades de que os dados podem ser salvos no formato yml, como demonstra a figura <<fig_yml>>.

[[fig_png, ImagensPng]]
.Saída do programa para a imagem PNG
image::images/imagempng.png[title="Dados no arquivo png."]

[[fig_yml, ImagensYML]]
.Saída do programa para o arquivo yml
image::images/imagemyml.png[title="Dados no arquivo yml."]

== Atividade 3: Esteganografia de uma imagem

Partindo de uma imagem que possuía uma segunda imagem escondida, esta atividade retorna a manipulação de pixels. Dessa vez é necessário fazer o movimento contrário do qual foi feita para se esconder a imagem. 

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

int main(int argc, char**argv) {
  	cv::Mat imagemPortadora, imagemEscondida, imagemFinal;
 	cv::Vec3b valPortadora, valEscondida, valFinal;
	int nbits = 3;

	imagemFinal = cv::imread(argv[1], cv::IMREAD_COLOR);
	
	imagemPortadora = imagemFinal.clone();
	imagemEscondida = imagemFinal.clone();

  	if (imagemFinal.empty()) 
	{
    	std::cout << "imagem nao carregou corretamente" << std::endl;
    	return (-1);
  	}

  	for (int i = 0; i < imagemFinal.rows; i++) 
	{
    	for (int j = 0; j < imagemFinal.cols; j++) 
		{
			valFinal = imagemFinal.at<cv::Vec3b>(i, j);
		
      		valPortadora[0] = valFinal[0] >> nbits << nbits;
      		valPortadora[1] = valFinal[1] >> nbits << nbits;
      		valPortadora[2] = valFinal[2] >> nbits << nbits;

     		valEscondida[0] = valFinal[0] << (8-nbits);
      		valEscondida[1] = valFinal[1] << (8-nbits);
      		valEscondida[2] = valFinal[2] << (8-nbits);
			
			imagemPortadora.at<cv::Vec3b>(i,j) = valPortadora;
			imagemEscondida.at<cv::Vec3b>(i,j) = valEscondida;	
    	}
  	}

	cv::imshow("Portadora",imagemPortadora);
	cv::waitKey();
	cv::imshow("Escondida",imagemEscondida);
  	cv::waitKey();

  return 0;
}
----

O procedimento é simplesmente pegar os n dígitos mais significativos da imagem e separá-los para uma imagem e pegar os Nbytes - n dígitos menos significativos e colocá-los em uma outra imagem. Como estamos utilizando imagens de 1 byte e um n de 3 foi o suficiente para revelar a imagem escondida, temos então os 5 bits mais significativos da imagem separados dos 3 bits menos significativos. Assim podemos separar as duas imagens como pode ser visto na figura <<fig_esteg>>.

[[fig_esteg, estego]]
.Saída do programa de separação de imagens
image::images/esteg.png[title="Imagem portadora e imagem escondida"]



== Atividade 4: Labeling de Imagens

Para se fazer o labeling primeiro se utilizou de floodfill para fazer a separação dos mais diversos tipos de figuras na tela. Como primeira experiência para resolver todos os problemas, se resolveu implementar um floodfill próprio, não se utilizando da api do opencv, para que assim fosse possível fazer as diversas distinções esperadas pelo problema. Esse código pode ser visto logo abaixo.

[source,cpp]
----
void myfloodFill(cv::Mat img, int r, int c, int targetColor, int newColor, bool* borda)
{
    if (r < 0 || r >= img.rows || c < 0 || c >= img.cols || img.at<uchar>(r,c) != targetColor) return;
    if(r == 0 || c == 0 || r == img.rows-1 || c == img.cols-1) *borda = true;

    img.at<uchar>(r,c) = newColor;

    myfloodFill(img, r+1, c, targetColor, newColor,borda);
    myfloodFill(img, r-1, c, targetColor, newColor,borda);
    myfloodFill(img, r, c+1, targetColor, newColor,borda);
    myfloodFill(img, r, c-1, targetColor, newColor,borda);
}
----

Dessa forma seria possível continuar com essa linha de pensamento, porém, o openCV dispões de formas muito mais eficientes de resolver esse problema. Utilizando qualquer floodfill para se livrar das bolhas que tocam as bordas e em seguida passando pela função findContours do OpenCV.

[source,cpp]
----
    std::vector<std::vector<cv::Point> > contours;
    std::vector<cv::Vec4i> hierarchy;
    cv::findContours(image, contours, hierarchy, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE);
     for(unsigned i = 0; i< hierarchy.size();i++)
    {
      if(hierarchy[i][3]<0 && hierarchy[i][2]<0)
      {
          solido++;
      }
      else if(hierarchy[i][3]>0)
      {
          buraco++;
      }
    }
----

Utilizando da lógica hierárquica recebida no vetor hierarchy é possível, então, definir quais bolhas são internas e as bolhas são externas. Com essa lógica é possível resolver a limitação máxima do labeling, assim como, definir bolhas internas a qualquer nível, ou seja, é possível entender qual grau de interno é uma bolha mesmo que tenhamos dezenas de bolhas se englobando. É ainda possível se utilizar de métricas do próprio openCV para calcular médias, detectar figuras, entre outros fins. Abaixo podemos ver o código completo.
[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

void myfloodFill(cv::Mat img, int r, int c, int targetColor, int newColor, bool* borda)
{
    if (r < 0 || r >= img.rows || c < 0 || c >= img.cols || img.at<uchar>(r,c) != targetColor) return;
    if(r == 0 || c == 0 || r == img.rows-1 || c == img.cols-1) *borda = true;

    img.at<uchar>(r,c) = newColor;

    myfloodFill(img, r+1, c, targetColor, newColor,borda);
    myfloodFill(img, r-1, c, targetColor, newColor,borda);
    myfloodFill(img, r, c+1, targetColor, newColor,borda);
    myfloodFill(img, r, c-1, targetColor, newColor,borda);
}


int main(int argc, char** argv) {
  cv::Mat image;
  int width, height;

  image = cv::imread(argv[1], cv::IMREAD_GRAYSCALE);

  if (!image.data) {
    std::cout << "imagem nao carregou corretamente\n";
    return (-1);
  }

  width = image.cols;
  height = image.rows;
  std::cout << width << "x" << height << std::endl;

  // Preprocessando Imagem
  
  for (int i = 0; i < height; i++) {
    for (int j = 0; j < width; j++) {
      if (image.at<uchar>(i, j) == 255) {
        bool borda = false;

        myfloodFill(image, i,j,255,254,&borda);
        if(borda)
        {
            myfloodFill(image,i,j,254,0,&borda);
        }
        else
        {
            myfloodFill(image,i,j,254,255,&borda);
        }
      }
    }
  }
  
  int buraco = 0;
  int solido = 0;

  // Utilizando busca de contornos da openCV
  //
  std::vector<std::vector<cv::Point> > contours;
  std::vector<cv::Vec4i> hierarchy;
  cv::findContours(image, contours, hierarchy, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE);
  
  for(unsigned i = 0; i< hierarchy.size();i++)
  {
      if(hierarchy[i][3]<0 && hierarchy[i][2]<0)
      {
          solido++;
      }
      else if(hierarchy[i][3]>0)
      {
          buraco++;
      }
  }

  std::cout << "a figura tem " << buraco+solido << " bolhas\n";
  std::cout << "Sendo " << buraco << " com buracos\n";
  std::cout << "Sendo " << solido << " solidas\n";
  
  std::vector<cv::Moments> mu(contours.size());
  for (size_t i = 0; i< contours.size();i++)
  {
      mu[i] = cv::moments(contours[i]);
  }

   for( size_t i = 0; i < contours.size(); i++ )
 {
     if(hierarchy[i][3]<0 && hierarchy[i][2]<0)
     {
        std::cout <<  "SOLIDO: " << i<< std::endl;;
        std::cout << "Area (M_00) = " << std::fixed << std::setprecision(2) << mu[i].m00 << std::endl<<std::endl;
     }
     else if(hierarchy[i][3]>0)
     {
        std::cout << "Com Buraco: " <<i<< std::endl;
        std::cout <<  "Area (M_00) = " << std::fixed << std::setprecision(2) << mu[i].m00<<std::endl<<std::endl;
     }
     else
     {
        std::cout << "Buraco: " <<i<< std::endl;
        std::cout <<  "Area (M_00) = " << std::fixed << std::setprecision(2) << mu[i].m00<<std::endl<<std::endl;
     }
 }

  cv::drawContours(image,contours,-1,127,2);

  cv::imshow("image", image);
  cv::imwrite("labeling.png", image);
  cv::waitKey();
  return 0;
}
----

A saída desse programa pode ser visto na figura <<fig_label>>

[[fig_label, labeling]]
.Saída do programa de labeling
image::images/label.png[title="Saída do programa de labeling"]


== Atividade 5: Histogramas

Trabalhar com imagens capturadas por uma câmera de vídeo acaba se tornando em uma série de trabalhos em imagens estáticas que deve ser feita de forma muito rápida. Nos programas dessa atividade é necessário processar em cima das imagens recebidas por uma câmera, sendo a primeira delas uma operação de equalização que tenta filtrar as mudanças bruscas de iluminação, enquanto para a segunda teremos de desenvolver um sensor de movimento.

=== Equalização

A equalização é feita de forma simples, após obter a imagem e transformá-la em tons de cinza essa deve ser normalizada com o comando normalize, isso deve ser feito nos 3 canais de cor. Em seguida basta mostrar a saída dessa imagem, podemos ver no código logo abaixo.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

int main(int argc, char** argv)
{
  cv::Mat image;
  int width, height;
  cv::VideoCapture cap;
  std::vector<cv::Mat> planes;
  cv::Mat histR, histG, histB;
  int nbins = 64;
  float range[] = {0, 255};
  const float *histrange = { range };
  bool uniform = true;
  bool acummulate = false;
  int key;

  cap.open(0);
  
  if(!cap.isOpened()){
    std::cout << "cameras indisponiveis";
    return -1;
  }
  
  cap.set(cv::CAP_PROP_FRAME_WIDTH, 640);
  cap.set(cv::CAP_PROP_FRAME_HEIGHT, 480);  
  width = cap.get(cv::CAP_PROP_FRAME_WIDTH);
  height = cap.get(cv::CAP_PROP_FRAME_HEIGHT);

  std::cout << "largura = " << width << std::endl;
  std::cout << "altura  = " << height << std::endl;

  int histw = nbins, histh = nbins/2;
  cv::Mat histImgR(histh, histw, CV_8UC3, cv::Scalar(0,0,0));
  cv::Mat histImgG(histh, histw, CV_8UC3, cv::Scalar(0,0,0));
  cv::Mat histImgB(histh, histw, CV_8UC3, cv::Scalar(0,0,0));

  while(1){
    cap >> image;
    cv::split (image, planes);
    cv::calcHist(&planes[0], 1, 0, cv::Mat(), histB, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    cv::calcHist(&planes[1], 1, 0, cv::Mat(), histG, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    cv::calcHist(&planes[2], 1, 0, cv::Mat(), histR, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    
    cv::normalize(histR, histR, 0, histImgR.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histG, histG, 0, histImgG.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histB, histB, 0, histImgB.rows, cv::NORM_MINMAX, -1, cv::Mat());
    
    histImgR.setTo(cv::Scalar(0));
    histImgG.setTo(cv::Scalar(0));
    histImgB.setTo(cv::Scalar(0));
    
    for(int i=0; i<nbins; i++){
      cv::line(histImgR,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histR.at<float>(i))),
               cv::Scalar(0, 0, 255), 1, 8, 0);
      cv::line(histImgG,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histG.at<float>(i))),
               cv::Scalar(0, 255, 0), 1, 8, 0);
      cv::line(histImgB,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histB.at<float>(i))),
               cv::Scalar(255, 0, 0), 1, 8, 0);
    }
    histImgR.copyTo(image(cv::Rect(0, 0       ,nbins, histh)));
    histImgG.copyTo(image(cv::Rect(0, histh   ,nbins, histh)));
    histImgB.copyTo(image(cv::Rect(0, 2*histh ,nbins, histh)));
	cv::Mat gray;
	cv::cvtColor(image,gray,cv::COLOR_BGR2GRAY);
	
	cv::Mat dst;
	cv::equalizeHist(gray,dst);
    cv::imshow("image", dst);
    key = cv::waitKey(30);
    if(key == 27) break;
    return 0;
}
----

A equalização torna a imagem mais resistente às mudanças e, portanto, torna a imagem mais estável. A imagem pode ser vista na figura <<fig_equal>>.


[[fig_equal, equalize]]
.Saída do programa de equalização
image::images/equal.png[title="Saída do programa de Equalização da câmera"]

=== Sensor de presença

Para desenvolver um sensor de presença devemos trabalhar com o conceito de média de uma imagem. Para essa atividade se mantém o desenvolvimento da média de forma constante a cada frame, sempre guardando a média das últimas imagens e o delta da imagem anterior para a média. Quando esse delta ultrapassa um limiar, no caso 10 em qualquer escala de cor, o sistema lança um aviso de alarme escrito e salva uma imagem do que foi capturada pelo sensor de presença. Abaixo podemos ver o código desse sensor.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>
#include <cmath>

int main(int argc, char** argv){
  cv::Mat image;
  int width, height;
  cv::VideoCapture cap;
  std::vector<cv::Mat> planes;
  cv::Mat histR, histG, histB;
  int nbins = 64;
  float range[] = {0, 255};
  const float *histrange = { range };
  bool uniform = true;
  bool acummulate = false;
  int key,number = 0;
  float mRed,mGreen,mBlue;
  float dRed,dGreen,dBlue;
  float lRed = 0.0;
  float lGreen = 0.0;
  float lBlue = 0.0;
  

  cap.open(0);
  
  if(!cap.isOpened()){
    std::cout << "cameras indisponiveis";
    return -1;
  }
  
  cap.set(cv::CAP_PROP_FRAME_WIDTH, 640);
  cap.set(cv::CAP_PROP_FRAME_HEIGHT, 480);  
  width = cap.get(cv::CAP_PROP_FRAME_WIDTH);
  height = cap.get(cv::CAP_PROP_FRAME_HEIGHT);

  std::cout << "largura = " << width << std::endl;
  std::cout << "altura  = " << height << std::endl;

  int histw = nbins, histh = nbins/2;
  cv::Mat histImgR(histh, histw, CV_8UC3, cv::Scalar(0,0,0));
  cv::Mat histImgG(histh, histw, CV_8UC3, cv::Scalar(0,0,0));
  cv::Mat histImgB(histh, histw, CV_8UC3, cv::Scalar(0,0,0));

  while(1){
    cap >> image;

    cv::split (image, planes);
    cv::calcHist(&planes[0], 1, 0, cv::Mat(), histB, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    cv::calcHist(&planes[1], 1, 0, cv::Mat(), histG, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    cv::calcHist(&planes[2], 1, 0, cv::Mat(), histR, 1,
                 &nbins, &histrange,
                 uniform, acummulate);
    
    cv::normalize(histR, histR, 0, histImgR.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histG, histG, 0, histImgG.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histB, histB, 0, histImgB.rows, cv::NORM_MINMAX, -1, cv::Mat());
    
    histImgR.setTo(cv::Scalar(0));
    histImgG.setTo(cv::Scalar(0));
    histImgB.setTo(cv::Scalar(0));
    
    for(int i=0; i<nbins; i++){
      cv::line(histImgR,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histR.at<float>(i))),
               cv::Scalar(0, 0, 255), 1, 8, 0);
      cv::line(histImgG,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histG.at<float>(i))),
               cv::Scalar(0, 255, 0), 1, 8, 0);
      cv::line(histImgB,
               cv::Point(i, histh),
               cv::Point(i, histh-cvRound(histB.at<float>(i))),
               cv::Scalar(255, 0, 0), 1, 8, 0);
    }
    histImgR.copyTo(image(cv::Rect(0, 0       ,nbins, histh)));
    histImgG.copyTo(image(cv::Rect(0, histh   ,nbins, histh)));
    histImgB.copyTo(image(cv::Rect(0, 2*histh ,nbins, histh)));
    
    cv::Scalar amean = cv::mean(image);
    mRed = amean.val[0];
    mGreen = amean.val[1];
    mBlue = amean.val[2];
    
    dRed = mRed - lRed;
    dGreen = mGreen - lGreen;
    dBlue = mBlue - lBlue;

    lRed = mRed;
    lGreen = mGreen;
    lBlue = mBlue;

    if(dRed>10 || dGreen>10 || dBlue>10)
    {
        std::cout<<"-----------ALARM-------------"<<std::endl;
        std::stringstream ss_img;
        ss_img<<"Alarms/Alarm"<<number++<<".png";
        cv::imwrite(ss_img.str(),image);
    }

    
    cv::imshow("image", image);
    key = cv::waitKey(30);
    if(key == 27) break;
  }
  return 0;
}
----

O funcionamento desse sistema de alarme pode ser visto na figura <<fig_alarm>>. Com um simples acesso a uma api de timetag é possível guardar os momentos de acontecimento dos alarmes e tornar este um sistema funcional de baixo custo.


[[fig_alarm, alarme]]
.Saída do programa de alarme
image::images/alarm.png[title="Saída do Programa de Sensor de Movimento"]


== Atividade 6: Filtros no domínio espacial

Para o desenvolvimento da atividade proposta basta que façamos uma segunda máscara e apliquemos em séria com a máscara aplicada anteriormente. Com a utilização do botão z como forma de aplicar o laplaciano a qualquer filtro colocado anteriormente podemos ver a aplicação posterior do lapalaciano a qualquer filtro. O laplaciano é um detector de bordas e por isso deve evidenciar as bordas de quaisquer figuras. Abaixo podemos ver o código completo que faz esse trabalho.

[source,cpp]
----
#include <iostream>
#include <opencv2/opencv.hpp>

void printmask(cv::Mat &m) {
  for (int i = 0; i < m.size().height; i++) {
    for (int j = 0; j < m.size().width; j++) {
      std::cout << m.at<float>(i, j) << ",";
    }
    std::cout << "\n";
  }
}

int main(int, char **) {
  cv::VideoCapture cap;  // open the default camera
  float media[] = {0.1111, 0.1111, 0.1111, 0.1111, 0.1111,
                   0.1111, 0.1111, 0.1111, 0.1111};
  float gauss[] = {0.0625, 0.125,  0.0625, 0.125, 0.25,
                   0.125,  0.0625, 0.125,  0.0625};
  float horizontal[] = {-1, 0, 1, -2, 0, 2, -1, 0, 1};
  float vertical[] = {-1, -2, -1, 0, 0, 0, 1, 2, 1};
  float laplacian[] = {0, -1, 0, -1, 4, -1, 0, -1, 0};
  float boost[] = {0, -1, 0, -1, 5.2, -1, 0, -1, 0};

  bool laplacegoogles = false;

  cv::Mat frame, framegray, frame32f, frameFiltered, frameFinal;
  cv::Mat mask(3, 3, CV_32F);
  cv::Mat mask2(3, 3, CV_32F);
  cv::Mat result;
  double width, height;
  int absolut;
  char key;

  cap.open(0);

  if (!cap.isOpened())  // check if we succeeded
    return -1;

  cap.set(cv::CAP_PROP_FRAME_WIDTH, 640);
  cap.set(cv::CAP_PROP_FRAME_HEIGHT, 480);
  width = cap.get(cv::CAP_PROP_FRAME_WIDTH);
  height = cap.get(cv::CAP_PROP_FRAME_HEIGHT);
  std::cout << "largura=" << width << "\n";
  ;
  std::cout << "altura =" << height << "\n";
  ;
  std::cout << "fps    =" << cap.get(cv::CAP_PROP_FPS) << "\n";
  std::cout << "format =" << cap.get(cv::CAP_PROP_FORMAT) << "\n";

  cv::namedWindow("filtroespacial", cv::WINDOW_NORMAL);
  cv::namedWindow("original", cv::WINDOW_NORMAL);

  mask = cv::Mat(3, 3, CV_32F, media);
  mask2 = cv::Mat(3, 3, CV_32F, media);

  absolut = 1;  // calcs abs of the image

  for (;;) {
    cap >> frame;  // get a new frame from camera
    cv::cvtColor(frame, framegray, cv::COLOR_BGR2GRAY);
    cv::flip(framegray, framegray, 1);
    cv::imshow("original", framegray);
    framegray.convertTo(frame32f, CV_32F);
    cv::filter2D(frame32f, frameFiltered, frame32f.depth(), 
    mask,
                 cv::Point(1, 1), 0);
    cv::filter2D(frameFiltered, frameFinal, frameFiltered.depth(), 
    mask2,
                cv::Point(1,1),0);
    if (absolut) {
      frameFinal = cv::abs(frameFinal);
    }

    frameFinal.convertTo(result, CV_8U);

    cv::imshow("filtroespacial", result);

    key = (char)cv::waitKey(10);
    if (key == 27) break;  // esc pressed!
    switch (key) {
      case 'a':
        absolut = !absolut;
        break;
      case 'm':
        mask = cv::Mat(3, 3, CV_32F, media);
        printmask(mask);
        break;
      case 'g':
        mask = cv::Mat(3, 3, CV_32F, gauss);
        printmask(mask);
        break;
      case 'h':
        mask = cv::Mat(3, 3, CV_32F, horizontal);
        printmask(mask);
        break;
      case 'v':
        mask = cv::Mat(3, 3, CV_32F, vertical);
        printmask(mask);
        break;
      case 'l':
        mask = cv::Mat(3, 3, CV_32F, laplacian);
        printmask(mask);
        break;
      case 'b':
        mask = cv::Mat(3, 3, CV_32F, boost);
        break;
      case 'z':
        laplacegoogles = !laplacegoogles;
        if(laplacegoogles)
        {
            mask2 = cv::Mat(3,3, CV_32F, laplacian);
        }
        else
        {
            mask2 = cv::Mat(3, 3, CV_32F, media);
        }
        break;
      default:
        break;
    }
  }
  return 0;
}

----

Com a aplicação de um filtro detector de bordas podemos ver que a saída na figura <<fig_laplace>> apenas com o filtro de laplace temos uma detecção das bordas em todas direções. 

[[fig_laplace, laplace]]
.Saída do programa de laplaciano
image::images/laplace.png[title="Saída do filtro laplaciano"]

Porém, com a utilização de um filtro de gauss e um laplaciano, como na figura <<fig_glaplace>>, temos uma acentuação ainda maior das bordas.

[[fig_glaplace, glaplace]]
.Saída do programa de gauss laplaciano
image::images/laplacegauss.png[title="Saída do filtro Gauss-laplaciano"]


== Atividade 7: Filtros Espaciais Tilt Shift Digital

Para desenvolver o tilt shift é necessário se utilizar de uma função de decaimento. No caso utilizamos de um decaimento hiperbólico. A função de tilt shift desenvolvida foi como abaixo, capaz de ter um nível de foco em qualquer lugar da imagem e um decaimento centralizado desse foco. Para se desenvolver o posicionamento da máscara de tiltshift ainda se utilizou a ideia de matrizes de translação, essa matriz faz com que a máscara se movimente para qualquer ponto na imagem. Para representar de forma fiel foram desenvolvidos 4 sliders: 2 que denotam a posição do centro do foco, 1 que denota a força do decaimento e 1 que denota o tamanho do foco. 

[source,cpp]
----
cv::Mat tiltshift(cv::Mat image,float unfocus, int decay, int offsetx, int offsety)
{
  cv::Mat g_image = cv::Mat::zeros(image.rows, image.cols, CV_32FC3);
  cv::Mat f_image = cv::Mat::zeros(image.rows, image.cols, CV_32FC3);
  cv::Mat unf_image = cv::Mat::zeros(image.rows, image.cols, CV_32FC3);
  cv::Mat f_mask = cv::Mat(image.rows, image.cols, CV_32FC3, cv::Scalar(0, 0, 0));
  cv::Mat unf_mask = cv::Mat(image.rows, image.cols, CV_32FC3, cv::Scalar(1, 1, 1));
  cv::Mat outp = cv::Mat::zeros(image.rows, image.cols, CV_32FC3);
  cv::Vec3f v_output, h_output, f_output;

  int l1 = unfocus*image.rows;
  int l2 = (1-unfocus) * image.rows;

  int c1 = unfocus*image.cols;
  int c2 = (1 - unfocus) * image.cols;

  for (int i = 0; i < image.rows; i++) 
  {
    for (int j = 0; j < image.cols; j++) 
    {
      h_output[0] = (tanh((float(i - l1) / decay)) - tanh((float(i - l2) / decay))) / 2;
      h_output[1] = (tanh((float(i - l1) / decay)) - tanh((float(i - l2) / decay))) / 2;
      h_output[2] = (tanh((float(i - l1) / decay)) - tanh((float(i - l2) / decay))) / 2;

      v_output[0] = (tanh((float(j - c1) / decay)) - tanh((float(j - c2) / decay))) / 2;
      v_output[1] = (tanh((float(j - c1) / decay)) - tanh((float(j - c2) / decay))) / 2;
      v_output[2] = (tanh((float(j - c1) / decay)) - tanh((float(j - c2) / decay))) / 2;

      f_output[0] = std::min(h_output[0],v_output[0]);
      f_output[1] = std::min(h_output[1],v_output[1]);
      f_output[2] = std::min(h_output[2],v_output[2]);

      f_mask.at<cv::Vec3f>(i, j) = f_output;
    }
  }

  translateImg(f_mask, offsetx, offsety);

  unf_mask = unf_mask - f_mask;

  image.convertTo(image, CV_32FC3);
  cv::GaussianBlur(image, g_image, cv::Size(15, 15), 0, 0);

  f_image = image.mul(f_mask);
  unf_image = g_image.mul(unf_mask);

  outp = f_image + unf_image;

  image.convertTo(image, CV_8UC3);
  outp.convertTo(outp, CV_8UC3);

  return outp;
}

cv::Mat translateImg(cv::Mat &img, int offsetx, int offsety)
{
    cv::Mat trans_mat = (cv::Mat_<double>(2,3) << 1, 0, offsety, 0, 1, offsetx);
    cv::warpAffine(img,img,trans_mat,img.size());
    return img;
}
----

Na entrada temos a figura <<fig_tiltin>>:

[[fig_tiltin, tiltin]]
.Entrada do programa de tilt in
image::images/tiltin.png[title="Fonte: https://g1.globo.com/sp/campinas-regiao/terra-da-gente/noticia/2021/09/29/conheca-a-arvore-rosa-de-30-metros-que-e-invisivel-para-muita-gente.ghtml"]


O programa funciona desta forma, como demonstrado na figura <<fig_tiltprog>>:

[[fig_tiltprog, tiltprog]]
.Saída do programa de tiltprog
image::images/tiltprog.png[title="Interface Programa Tiltshift"]


Como saída do programa temos a figura <<fig_tiltout>>:

[[fig_tiltout, tiltout]]
.Saída do programa de tilt
image::images/tiltout.jpg[title="Saída do Programa de Tiltshift"]


O código completo se encontra em link:exemplos/tiltshift.cpp[]

=== Aplicação do tiltshift em vídeo.

Para desenvolver o tilt shift em um vídeo qualquer basta que utilizemos a função feita na atividade anterior e apliquemos a cada frame que quisermos. Temos ainda de gerar o efeito de stop motion para isso iremos pegar apenas 1 imagem a cada iterações do programa. No caso se utilizou um valor de 4, ou seja, 1 a cada 4 frames serão ignorados. Uma vez que todo processamento seja feito, o programa irá salvar o que foi produzido.

[source,cpp]
----
#include <opencv2/opencv.hpp>
#include <iostream>
#include <cstdio>
#include <math.h>
#include <stdio.h>
#include <string>

#define IGNORE_FRAMES 4

cv::Mat tiltshift(cv::Mat image,float unfocus, int decay, int offsetx, int offsety);
cv::Mat translateImg(cv::Mat &img, int offsetx, int offsety);

int main(int argvc, char** argv)
{
  cv::VideoCapture vid_capture(argv[1]);
  cv::VideoWriter outputVideo;

  int ex = static_cast<int>(vid_capture.get(cv::CAP_PROP_FOURCC));  
  cv::Size S = cv::Size((int) vid_capture.get(cv::CAP_PROP_FRAME_WIDTH),    // Acquire input size
              (int) vid_capture.get(cv::CAP_PROP_FRAME_HEIGHT));

  if (!vid_capture.isOpened())
  {
    std::cout << "Error opening video stream or file" << std::endl;
  }
  else
  {
    outputVideo.open("Tilted Output.mp4", ex, vid_capture.get(cv::CAP_PROP_FPS), S, true);
    if (!outputVideo.isOpened())
    {
        std::cout  << "Could not open the output video for write: " << argv[1] << std::endl;
        return -1;
    }
  }

  int total_frame = 0;
  int frame_count = vid_capture.get(7)/IGNORE_FRAMES;

  int ftocap = 1;

  while (vid_capture.isOpened())
  {
    cv::Mat frame;
    bool isSuccess = vid_capture.read(frame);
    if(isSuccess == true)
    {
      if(--ftocap == 0)
      {
        outputVideo << tiltshift(frame,0.15,201,0,0);
        ftocap = IGNORE_FRAMES;
        std::cout << total_frame++ << "/" << frame_count << std::endl;
      }
    }
    else
    {
      break;
    } 
  }

  std::cout << "Finished Conversion" << std::endl;

  vid_capture.release();
  cv::destroyAllWindows();

  return 0;
}
----
Então podemos ver logo abaixo o resultado obtido por esse programa com o vídeo <<vid_tilt>>.
[[vid_tilt, tiltvid]]
.Original: https://www.youtube.com/watch?v=eHRDzsr-g4w
video::videos/dronenatal.mp4[]


== ------------ Unidade 2 ------------ 

Abaixo temos as atividades desenvolvidas para a unidade 2 da disciplina de Processamento Digital de Imagens.

== Atividade 8: Magnitude em Frequência

Através da transformada de fourier é possível analisar o espectro de frequência de um sinal, ao utilizar essa técnica em uma imagem é possível perceber a tendência periódica de uma imagem. O codigo a seguir foi montado para se retirar a tendência períodica de uma imagem, e caso se utilize o yml ao inves do nome da imagem, ele lerá o arquivo senoide-256.yml.

[source,cpp]
----
#include <iostream>
#include <vector>
#include <string>
#include <opencv2/opencv.hpp>

void swapQuadrants(cv::Mat& image) {
  cv::Mat tmp, A, B, C, D;

  // se a imagem tiver tamanho impar, recorta a regiao para o maior
  // tamanho par possivel (-2 = 1111...1110)
  image = image(cv::Rect(0, 0, image.cols & -2, image.rows & -2));

  int centerX = image.cols / 2;
  int centerY = image.rows / 2;

  // rearranja os quadrantes da transformada de Fourier de forma que 
  // a origem fique no centro da imagem
  // A B   ->  D C
  // C D       B A
  A = image(cv::Rect(0, 0, centerX, centerY));
  B = image(cv::Rect(centerX, 0, centerX, centerY));
  C = image(cv::Rect(0, centerY, centerX, centerY));
  D = image(cv::Rect(centerX, centerY, centerX, centerY));

  // swap quadrants (Top-Left with Bottom-Right)
  A.copyTo(tmp);
  D.copyTo(A);
  tmp.copyTo(D);

  // swap quadrant (Top-Right with Bottom-Left)
  C.copyTo(tmp);
  B.copyTo(C);
  tmp.copyTo(B);
}

int main(int argc, char** argv) {
  cv::Mat image, padded, complexImage;
  std::vector<cv::Mat> planos;

  if (std::string(argv[1]) == "yml") 
  {
    cv::FileStorage fs("senoide-256.yml", cv::FileStorage::READ);

    fs["mat"] >> image;

    fs.release();
  }
  else
  {
    image = imread(argv[1], cv::IMREAD_GRAYSCALE);
    if (image.empty()) {
      std::cout << "Erro abrindo imagem" << argv[1] << std::endl;
      return EXIT_FAILURE;
    }
  }

  
  // expande a imagem de entrada para o melhor tamanho no qual a DFT pode ser
  // executada, preenchendo com zeros a lateral inferior direita.
  int dft_M = cv::getOptimalDFTSize(image.rows);
  int dft_N = cv::getOptimalDFTSize(image.cols); 
  cv::copyMakeBorder(image, padded, 0, dft_M - image.rows, 0, dft_N - image.cols, cv::BORDER_CONSTANT, cv::Scalar::all(0));

  // prepara a matriz complexa para ser preenchida
  // primeiro a parte real, contendo a imagem de entrada
  planos.push_back(cv::Mat_<float>(padded)); 
  // depois a parte imaginaria com valores nulos
  planos.push_back(cv::Mat::zeros(padded.size(), CV_32F));

  // combina os planos em uma unica estrutura de dados complexa
  cv::merge(planos, complexImage);  

  // calcula a DFT
  cv::dft(complexImage, complexImage); 
  swapQuadrants(complexImage);

  // planos[0] : Re(DFT(image)
  // planos[1] : Im(DFT(image)
  cv::split(complexImage, planos);

  // calcula o espectro de magnitude e de fase (em radianos)
  cv::Mat magn, fase;
  cv::cartToPolar(planos[0], planos[1], magn, fase, false);
  cv::normalize(fase, fase, 0, 1, cv::NORM_MINMAX);

  // caso deseje apenas o espectro de magnitude da DFT, use:
  cv::magnitude(planos[0], planos[1], magn); 

  // some uma constante para evitar log(0)
  // log(1 + sqrt(Re(DFT(image))^2 + Im(DFT(image))^2))
  magn += cv::Scalar::all(1);

  // calcula o logaritmo da magnitude para exibir
  // com compressao de faixa dinamica
  log(magn, magn);
  cv::normalize(magn, magn, 0, 1, cv::NORM_MINMAX);

  // exibe as imagens processadas
  cv::imshow("Imagem", image);  
  cv::imshow("Espectro de magnitude", magn);
  cv::imshow("Espectro de fase", fase);

  cv::waitKey();
  return EXIT_SUCCESS;
}
----

Quando entregamos como entrada para o programa a figura <<fig_sennorm>>:

[[fig_sennorm, Seno]]
image::images/senoide-256.png[title="Imagem de uma senóide salva"]

Na saída obtemos a figura <<fig_magsen>>:

[[fig_magsen, Seno yml]]
image::images/magsen.png[title="Resposta em magnitude da Imagem"]

Ja quando aplicamos de um arquivo yml obtemos a figura <<fig_magseny>>:

[[fig_magseny, Seno yml]]
image::images/magsenyml.png[title="Resposta em magnitude do YML"]

Como foi possível ver quando fazemos a transformada de fourier na imagem representante da senoide obtemos uma senoide contaminada por várias frequências além daquela principal, apesar desta ser dominante, ainda existem várias frequências menores observadas. Isso se dá, principalmente, pelo efeito de arredondamentos, por uma imagem apresentar apenas 8 bits de resolução obtemos uma resposta bem simplificada de uma senóide. Enquanto isso, quando utilizamos o arquivo yml e floats com uma resolução bem maior que apenas 8 bits, algo entre 32 e 64 bits, alcançamos uma precisão bem maior e temos a resposta que era esperada pela literatura.

== Atividade 9: Filtro Homomórfico

Uma aplicação para a transformada de fourier é o desenvolvimento de um filtro homomórfico capaz de melhorar a visualização de ambientes mal iluminados. Para isso é necessário se utilizar uma análise no ambiente de frequências. Uma vez que tenhamos um estado de decaimento do filtro e um raio mínimo podemos desenvolver a função como dada pela abaixo. onde a função homomorphicFilter desenvolve o filtro e se utiliza da função make filter para gerar a máscara de decaimento do filtro gerando, assim, um filtro não ideal que trabalha melhor com situações reais.
[source,cpp]
----

void makeFilter(const cv::Mat& image, cv::Mat& filter, double decayRate) {
    cv::Mat_<float> filter2D(image.rows, image.cols);
    int centerX = image.cols / 2;
    int centerY = image.rows / 2;

    for (int i = 0; i < image.rows; i++) {
        for (int j = 0; j < image.cols; j++) {
            double distance = std::sqrt(std::pow(i - centerY, 2) + std::pow(j - centerX, 2));
            
            double decay = std::exp(-decayRate * distance);

            if (distance <= radius) {
                filter2D.at<float>(i, j) = 1 - decay;
            } else {
                filter2D.at<float>(i, j) = 1;
            }
        }
    }

    cv::Mat planes[] = {cv::Mat_<float>(filter2D), cv::Mat::zeros(filter2D.size(), CV_32F)};
    cv::merge(planes, 2, filter);
}
void homomorphicFilter() {
    double decayRate = scaledDecayRate / 100.0; 
    cv::Mat image = originalImage.clone();  
    image.convertTo(image, CV_32F);

    cv::Mat logImage;
    cv::log(image + 1, logImage);


    cv::dft(logImage, image, cv::DFT_COMPLEX_OUTPUT);
    swapQuadrants(image);

    cv::Mat filter;
    makeFilter(image, filter, decayRate);

    cv::mulSpectrums(image, filter, image, 0);

    swapQuadrants(image);
    cv::idft(image, image, cv::DFT_SCALE | cv::DFT_REAL_OUTPUT);

    cv::exp(image, complexImage);
}
----

Uma vez que se tenha as funções principais para o funcionamento bastou desenvolver o código com sliders que apliquem diferentes parâmetros ao filtro para uma dada imagem como demonstra o código reduzido abaixo.

[source,cpp]
----
#include <iostream>
#include <vector>
#include <opencv2/opencv.hpp>

int radius = 15;        
int scaledDecayRate = 1; 

cv::Mat originalImage;  
cv::Mat complexImage;  

void swapQuadrants(cv::Mat& image) {
  cv::Mat tmp, A, B, C, D;

  // se a imagem tiver tamanho impar, recorta a regiao para o maior
  // tamanho par possivel (-2 = 1111...1110)
  image = image(cv::Rect(0, 0, image.cols & -2, image.rows & -2));

  int centerX = image.cols / 2;
  int centerY = image.rows / 2;

  // rearranja os quadrantes da transformada de Fourier de forma que 
  // a origem fique no centro da imagem
  // A B   ->  D C
  // C D       B A
  A = image(cv::Rect(0, 0, centerX, centerY));
  B = image(cv::Rect(centerX, 0, centerX, centerY));
  C = image(cv::Rect(0, centerY, centerX, centerY));
  D = image(cv::Rect(centerX, centerY, centerX, centerY));

  // swap quadrants (Top-Left with Bottom-Right)
  A.copyTo(tmp);
  D.copyTo(A);
  tmp.copyTo(D);

  // swap quadrant (Top-Right with Bottom-Left)
  C.copyTo(tmp);
  B.copyTo(C);
  tmp.copyTo(B);
}

void makeFilter(const cv::Mat& image, cv::Mat& filter, double decayRate) 
{
(...)
}
void homomorphicFilter() {
(...)
}

void onSlider(int, void* userdata) 
{
    homomorphicFilter();  
    cv::normalize(complexImage, complexImage, 0, 1, cv::NORM_MINMAX);

    cv::imshow("Homomorphic Filter", complexImage);

    cv::waitKey(1); 
}

int main(int argc, char** argv) {
    cv::Mat image, padded;
    std::vector<cv::Mat> planos;

    image = cv::imread(argv[1], cv::IMREAD_GRAYSCALE);
    if (image.empty()) {
        std::cout << "Error opening image " << argv[1] << std::endl;
        return EXIT_FAILURE;
    }

    originalImage = image.clone(); 

    int dft_M = cv::getOptimalDFTSize(image.rows);
    int dft_N = cv::getOptimalDFTSize(image.cols);
    cv::copyMakeBorder(image, padded, 0, dft_M - image.rows, 0, dft_N - image.cols, cv::BORDER_CONSTANT, cv::Scalar::all(0));

    planos.push_back(cv::Mat_<float>(padded));
    planos.push_back(cv::Mat::zeros(padded.size(), CV_32F));
    cv::merge(planos, complexImage);

    cv::namedWindow("Homomorphic Filter");
    cv::createTrackbar("Radius", "Homomorphic Filter", &radius, 100, onSlider);
    cv::createTrackbar("Decay Rate", "Homomorphic Filter", &scaledDecayRate, 100, onSlider);

    // Initial display
    onSlider(radius, nullptr);

    cv::waitKey(0);
    return EXIT_SUCCESS;
}
----

Quando entregamos como entrada para o programa a figura <<fig_Light>>:

[[fig_Light, Ambiente Mal Iluminado]]
image::images/Light.png[title="Imagem de um ambiente mal iluminado"]

Temos a saída como dada pelo video da <<vid_hmf>> abaixo:

[[vid_hmf, Imagem Com Slider]]
video::videos/hmf.mp4[]

No vídeo acima é possível observar que parâmetros diferentes melhoram partes diferentes da imagem e permitem a observação de espaços que eram muito menos visíveis na imagem original.

== Atividade 10: Algoritmo de Arte Pontilhista

A detecção de bordas é essencial para a análise de imagem de forma geral. Como uma forma de desenvolver a lógica desse tipo de algoritmo se resolveu aplicar os conhecimentos a produção de uma arte pontilhista a partir de uma imagem dada. Se optou então por um algoritmo bem curto, simples e efetivo na produção de imagens pontilhistas em RGB. O algoritmo se baseia na simples lógica:

1. Se define as repinturas e o nível de espalhamento da peça.
2. Se faz a captura da imagem.
3. Se começa a iteração dependendo no numero de repinturas.
4. Aplicando o algoritmo de canny com limiar crescente a cada iteração.
5. Se captura as cores presentes nos pontos de borda detectados. 
6. Se faz a pintura de pontos em uma posição proxima aleatoria dependente do espalhamento.
7. Se reduz o tamanho do pincel a cada iteração para que os pontos de alto detalhe sejam melhor pintados.


O código em c++ pode ser visto logo abaixo.

[source,cpp]
----
#include <iostream>
#include "opencv2/opencv.hpp"
#include <thread>

#define JITTER 3
#define PAINTINGS_OVER 10 //Numero de Pinturas sob Limites cada vez mais restritivos de CANNY

int width, height;
int x, y;
cv::Mat image, border, points;

int main(int argc, char** argv) {
    image = cv::imread(argv[1], cv::IMREAD_COLOR);
    width = image.cols;
    height = image.rows;

    cv::namedWindow("Point", 1);
    points = cv::Mat(height, width, CV_8UC3, cv::Scalar(255, 255, 255));

    for (int h = 0; h <= 100; h += 100/PAINTINGS_OVER) {
        cv::Canny(image, border, h, 3 * h);
        for (int i = 0; i < height; i++) {
            for (int j = 0; j < width; j++) {
                if (border.at<uchar>(i, j) == 255) 
                {
                    x = i + std::rand() % (2 * JITTER) - JITTER + 1;
                    y = j + std::rand() % (2 * JITTER) - JITTER + 1;

                    cv::Vec3b intensity = image.at<cv::Vec3b>(x, y);
                    int blue = intensity[0];
                    int green = intensity[1];
                    int red = intensity[2];

                    cv::circle(points, cv::Point(y, x), PAINTINGS_OVER + 1 - h / (100/PAINTINGS_OVER), CV_RGB(red, green, blue),
                               cv::FILLED, cv::LINE_AA);
                }
            }
        }
        cv::imshow("Point", points);
        cv::waitKey(100);
    }

    cv::waitKey();
    cv::imwrite("pontilism.png", points);
    return 0;
}
----

O processo de pintura foi capturado no gif abaixo:

[[fig_pgif, Pontilhismo]]
image::images/Arvre Point.gif[title="As pinturas do programa pontilhismo"]